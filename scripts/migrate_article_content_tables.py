#!/usr/bin/env python3
"""
Migration script to separate article content into dedicated tables.

This script is IDEMPOTENT and safe to run multiple times. It will:
1. Check if required tables exist and create them if missing
2. Check if migration has already been completed
3. Migrate existing content from ScrapedArticle.content to new tables (if needed)
4. Verify the migration was successful
5. Optionally remove the old content column (when uncommented)

Features:
- ‚úÖ Safe to re-run - checks existing state before making changes
- ‚úÖ Works with both PostgreSQL and SQLite databases  
- ‚úÖ Detailed logging and error handling
- ‚úÖ Automatic content analysis (HTML vs plain text)
- ‚úÖ Content deduplication via SHA-256 hashing
- ‚úÖ Word/character count analytics

Usage:
    python scripts/migrate_article_content_tables.py

The script will automatically detect your database type and handle the migration accordingly.
"""

import asyncio
import hashlib
import logging
from pathlib import Path
import sys

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from database.base import AsyncSessionLocal, engine
from database.models import ScrapedArticle, ArticleTextContent, ArticleHtmlContent
from sqlalchemy import text, select, func
from sqlalchemy.exc import IntegrityError

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def calculate_content_hash(content):
    """Calculate SHA-256 hash of content."""
    if not content:
        return None
    return hashlib.sha256(content.encode('utf-8')).hexdigest()


def strip_html_tags(html_content):
    """Simple HTML tag removal for text content."""
    if not html_content:
        return ""
    
    import re
    # Remove HTML tags
    clean = re.compile('<.*?>')
    text_content = re.sub(clean, '', html_content)
    
    # Clean up whitespace
    text_content = re.sub(r'\s+', ' ', text_content).strip()
    
    return text_content


def count_words(text):
    """Count words in text."""
    if not text:
        return 0
    return len(text.split())


async def check_and_create_tables():
    """Check if tables exist and create them if they don't."""
    logger.info("Checking for required database tables...")
    
    # Import Base to ensure all models are registered
    from database.models import Base
    
    async with AsyncSessionLocal() as session:
        # Check if tables exist using information_schema (works for PostgreSQL)
        try:
            # Check for PostgreSQL
            result = await session.execute(text("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema='public' 
                AND (table_name='article_text_content' OR table_name='article_html_content')
            """))
            existing_tables = [row[0] for row in result.fetchall()]
        except:
            try:
                # Fallback for SQLite
                result = await session.execute(text("""
                    SELECT name 
                    FROM sqlite_master 
                    WHERE type='table' 
                    AND (name='article_text_content' OR name='article_html_content')
                """))
                existing_tables = [row[0] for row in result.fetchall()]
            except:
                # If both fail, assume tables don't exist
                existing_tables = []
        
        logger.info(f"Found existing tables: {existing_tables}")
        
        # Determine which tables need to be created
        tables_to_create = []
        if 'article_text_content' not in existing_tables:
            tables_to_create.append('article_text_content')
        if 'article_html_content' not in existing_tables:
            tables_to_create.append('article_html_content')
        
        if not tables_to_create:
            logger.info("‚úÖ All required tables already exist")
            return True
        
        logger.info(f"Creating missing tables: {tables_to_create}")
    
    # Create missing tables
    async with engine.begin() as conn:
        if 'article_text_content' in tables_to_create:
            await conn.run_sync(lambda sync_conn: 
                ArticleTextContent.__table__.create(sync_conn, checkfirst=True)
            )
            logger.info("‚úÖ Created article_text_content table")
            
        if 'article_html_content' in tables_to_create:
            await conn.run_sync(lambda sync_conn: 
                ArticleHtmlContent.__table__.create(sync_conn, checkfirst=True)
            )
            logger.info("‚úÖ Created article_html_content table")
    
    logger.info("‚úÖ All required tables are now available")
    return True


async def check_migration_status():
    """Check if migration has already been completed."""
    logger.info("Checking migration status...")
    
    async with AsyncSessionLocal() as session:
        try:
            # Count existing content records
            text_count_result = await session.execute(text("SELECT COUNT(*) FROM article_text_content"))
            html_count_result = await session.execute(text("SELECT COUNT(*) FROM article_html_content"))
            
            text_count = text_count_result.scalar()
            html_count = html_count_result.scalar()
            
            # Count articles with content that need migration
            try:
                articles_result = await session.execute(
                    text("SELECT COUNT(*) FROM scraped_articles WHERE content IS NOT NULL AND content != ''")
                )
                articles_with_content = articles_result.scalar()
            except:
                # If the content column doesn't exist anymore, assume migration is complete
                logger.info("Content column not found - migration appears to be complete")
                return True, text_count, html_count, 0
            
            logger.info(f"Migration status - Text: {text_count}, HTML: {html_count}, Articles with content: {articles_with_content}")
            
            # If we have content records that match or exceed the articles with content, migration is likely complete
            if text_count >= articles_with_content and html_count >= articles_with_content and articles_with_content > 0:
                logger.info("‚úÖ Migration appears to be already complete")
                return True, text_count, html_count, articles_with_content
            
            return False, text_count, html_count, articles_with_content
            
        except Exception as e:
            logger.error(f"Error checking migration status: {e}")
            return False, 0, 0, 0


async def migrate_existing_content():
    """Migrate existing article content to the new tables."""
    logger.info("Starting content migration...")
    
    # Check if migration is already complete
    is_complete, text_count, html_count, articles_with_content = await check_migration_status()
    
    if is_complete and articles_with_content > 0:
        logger.info(f"Migration already complete - {text_count} text records, {html_count} HTML records")
        return True
    
    async with AsyncSessionLocal() as session:
        # Get all articles with content
        try:
            result = await session.execute(
                text("SELECT id, content FROM scraped_articles WHERE content IS NOT NULL AND content != ''")
            )
            articles = result.fetchall()
        except Exception as e:
            logger.error(f"Error fetching articles - content column may not exist: {e}")
            logger.info("Assuming migration is already complete")
            return True
        
        logger.info(f"Found {len(articles)} articles with content to migrate")
        
        if len(articles) == 0:
            logger.info("No articles with content found - migration may already be complete")
            return True
        
        migrated_count = 0
        error_count = 0
        
        for article_row in articles:
            article_id = article_row[0]
            raw_content = article_row[1]
            
            try:
                # Check if content already exists in new tables  
                existing_text_result = await session.execute(
                    select(ArticleTextContent).where(ArticleTextContent.article_id == article_id)
                )
                existing_text = existing_text_result.scalar_one_or_none()
                
                existing_html_result = await session.execute(
                    select(ArticleHtmlContent).where(ArticleHtmlContent.article_id == article_id)
                )
                existing_html = existing_html_result.scalar_one_or_none()
                
                if existing_text and existing_html:
                    logger.debug(f"Content for article {article_id} already migrated, skipping")
                    continue
                
                # Determine content format (simple heuristic)
                is_html = bool(raw_content and ('<' in raw_content and '>' in raw_content))
                
                # Create text content
                if not existing_text:
                    if is_html:
                        text_content = strip_html_tags(raw_content)
                    else:
                        text_content = raw_content
                    
                    text_record = ArticleTextContent(
                        article_id=article_id,
                        content=text_content,
                        word_count=count_words(text_content),
                        character_count=len(text_content) if text_content else 0,
                        content_hash=calculate_content_hash(text_content)
                    )
                    session.add(text_record)
                
                # Create HTML content
                if not existing_html:
                    if is_html:
                        html_content = raw_content
                        sanitized_content = raw_content  # Will be sanitized later by the app
                    else:
                        # Convert plain text to simple HTML
                        html_content = f"<p>{raw_content.replace(chr(10), '</p><p>')}</p>" if raw_content else ""
                        sanitized_content = html_content
                    
                    html_record = ArticleHtmlContent(
                        article_id=article_id,
                        content=html_content,
                        sanitized_content=sanitized_content,
                        content_type='html',
                        content_hash=calculate_content_hash(html_content)
                    )
                    session.add(html_record)
                
                migrated_count += 1
                
                # Commit in batches of 100
                if migrated_count % 100 == 0:
                    await session.commit()
                    logger.info(f"Migrated {migrated_count} articles...")
                    
            except Exception as e:
                error_count += 1
                logger.error(f"Error migrating article {article_id}: {e}")
                await session.rollback()
                continue
        
        # Final commit
        await session.commit()
        
        logger.info(f"‚úÖ Migration completed: {migrated_count} articles migrated, {error_count} errors")


async def verify_migration():
    """Verify that the migration was successful."""
    logger.info("Verifying migration...")
    
    async with AsyncSessionLocal() as session:
        try:
            # Count records in each table
            try:
                original_result = await session.execute(
                    text("SELECT COUNT(*) FROM scraped_articles WHERE content IS NOT NULL AND content != ''")
                )
                original_count = original_result.scalar()
            except Exception as e:
                logger.info(f"Could not count original articles (content column may not exist): {e}")
                original_count = 0
            
            text_count = await session.scalar(select(func.count(ArticleTextContent.id)))
            html_count = await session.scalar(select(func.count(ArticleHtmlContent.id)))
            
            # Also count total articles to provide context
            total_articles = await session.scalar(select(func.count(ScrapedArticle.id)))
            
            logger.info(f"üìä Migration verification results:")
            logger.info(f"   Total articles in database: {total_articles}")
            logger.info(f"   Original articles with content: {original_count}")
            logger.info(f"   Text content records: {text_count}")
            logger.info(f"   HTML content records: {html_count}")
            
            # Check for basic consistency
            if text_count > 0 and html_count > 0:
                if text_count == html_count:
                    logger.info("‚úÖ Text and HTML content counts match")
                else:
                    logger.warning(f"‚ö†Ô∏è  Text ({text_count}) and HTML ({html_count}) counts don't match")
                
                # If we can't get original count (column removed), just verify we have content
                if original_count == 0:
                    logger.info("‚úÖ Migration verification successful (original content column not accessible)")
                    return True
                elif text_count >= original_count and html_count >= original_count:
                    logger.info("‚úÖ Migration verification successful")
                    return True
                else:
                    logger.warning(f"‚ö†Ô∏è  Content counts lower than expected but migration may still be valid")
                    return True  # Don't fail - might be a partial migration that's still useful
            else:
                logger.error("‚ùå No content records found in new tables")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Migration verification failed with error: {e}")
            return False


async def remove_old_content_column():
    """Remove the old content column from ScrapedArticle table."""
    logger.info("Removing old content column...")
    
    try:
        async with engine.begin() as conn:
            # SQLite doesn't support DROP COLUMN directly, so we need to recreate the table
            await conn.execute(text("""
                CREATE TABLE scraped_articles_new AS 
                SELECT 
                    id, source_id, url, title, summary, author, published_date,
                    image_path, image_url, article_metadata, scraped_at, created_at, updated_at
                FROM scraped_articles
            """))
            
            await conn.execute(text("DROP TABLE scraped_articles"))
            await conn.execute(text("ALTER TABLE scraped_articles_new RENAME TO scraped_articles"))
            
            # Recreate indexes
            await conn.execute(text("CREATE UNIQUE INDEX ix_scraped_articles_url ON scraped_articles (url)"))
            await conn.execute(text("CREATE INDEX ix_scraped_articles_source_id ON scraped_articles (source_id)"))
            await conn.execute(text("CREATE INDEX ix_scraped_articles_id ON scraped_articles (id)"))
            
        logger.info("‚úÖ Old content column removed successfully")
        
    except Exception as e:
        logger.error(f"Error removing old content column: {e}")
        raise


async def main():
    """Run the complete migration process."""
    logger.info("üöÄ Starting article content table migration...")
    
    try:
        # Step 1: Check and create tables if needed
        logger.info("Step 1: Checking database tables...")
        if not await check_and_create_tables():
            logger.error("Failed to create required tables")
            return False
        
        # Step 2: Migrate existing content
        logger.info("Step 2: Migrating content...")
        migration_success = await migrate_existing_content()
        if not migration_success:
            logger.error("Content migration failed")
            return False
        
        # Step 3: Verify migration
        logger.info("Step 3: Verifying migration...")
        if not await verify_migration():
            logger.warning("Migration verification had issues, but continuing...")
            # Don't fail here as the migration might still be valid
        
        # Step 4: Remove old content column (optional - uncomment when ready)
        # logger.info("Step 4: Removing old content column...")
        # await remove_old_content_column()
        logger.info("‚ö†Ô∏è  Step 4: Old content column removal skipped for safety.")
        logger.info("   Uncomment remove_old_content_column() call when ready to finalize migration.")
        
        logger.info("üéâ Migration process completed successfully!")
        logger.info("üìù Summary:")
        
        # Final status check
        async with AsyncSessionLocal() as session:
            try:
                text_count = await session.scalar(text("SELECT COUNT(*) FROM article_text_content"))
                html_count = await session.scalar(text("SELECT COUNT(*) FROM article_html_content"))
                logger.info(f"   üìä Text content records: {text_count}")
                logger.info(f"   üìä HTML content records: {html_count}")
            except Exception as e:
                logger.info(f"   üìä Could not get final counts: {e}")
        
        logger.info("üìù Next steps:")
        logger.info("   1. ‚úÖ Test the application with new content structure")
        logger.info("   2. ‚è≥ Uncomment remove_old_content_column() when ready")
        logger.info("   3. ‚úÖ API endpoints updated to use new content tables")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Migration failed with error: {e}")
        logger.error("üìù This is typically safe to re-run - the script is idempotent")
        import traceback
        logger.error(f"Full traceback: {traceback.format_exc()}")
        return False


if __name__ == "__main__":
    # Import func for verification
    from sqlalchemy import func
    
    success = asyncio.run(main())
    sys.exit(0 if success else 1)