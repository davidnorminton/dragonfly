# Transcription Speed Optimization Guide

## Current Implementation
- **Service**: Vosk (offline, local)
- **Model**: vosk-model-en-us-0.22
- **Speed**: ~500-1000ms for typical AI Focus queries
- **Pros**: Free, offline, no API calls
- **Cons**: Slower than cloud alternatives, less accurate

---

## âš¡ Faster Alternatives

### 1. **Deepgram** (FASTEST - Recommended)
**Speed**: 100-300ms (3-5x faster than Vosk)
**Accuracy**: 95%+

#### Pros:
- âœ… Ultra-low latency (real-time streaming)
- âœ… Excellent accuracy
- âœ… Streaming support (get partial results immediately)
- âœ… Optimized for conversational AI
- âœ… WebSocket support
- âœ… Free tier: 200 hours/month

#### Cons:
- âŒ Requires API key
- âŒ Costs after free tier ($0.0043/min = ~$0.26/hour)

#### Pricing:
- Free tier: 200 hours/month
- After: $0.0043/minute (~$0.26/hour)
- For AI Focus: ~100 queries/day = ~5 hours/month (FREE)

**Recommendation: Best option for production**

---

### 2. **OpenAI Whisper API** (FAST)
**Speed**: 200-500ms (2-3x faster than Vosk)
**Accuracy**: 98%+

#### Pros:
- âœ… Best accuracy
- âœ… Handles accents, noise well
- âœ… Multiple languages
- âœ… Simple API

#### Cons:
- âŒ Requires OpenAI API key
- âŒ Higher cost ($0.006/minute = $0.36/hour)

#### Pricing:
- $0.006/minute ($0.36/hour)
- For AI Focus: ~100 queries/day = ~5 hours/month = ~$1.80/month

**Recommendation: Good if you already use OpenAI**

---

### 3. **AssemblyAI** (FAST)
**Speed**: 150-400ms (2-4x faster than Vosk)
**Accuracy**: 95%+

#### Pros:
- âœ… Real-time streaming
- âœ… Good accuracy
- âœ… WebSocket support
- âœ… Free tier: 5 hours/month

#### Cons:
- âŒ Requires API key
- âŒ Smaller free tier

#### Pricing:
- Free tier: 5 hours/month
- After: $0.00065/second (~$0.039/minute, $2.34/hour)

---

### 4. **Whisper.cpp (Local, Turbo)** (MODERATE)
**Speed**: 300-600ms (1.5-2x faster than Vosk)
**Accuracy**: 95%+

#### Pros:
- âœ… Free
- âœ… Offline
- âœ… Better than Vosk
- âœ… GPU acceleration support

#### Cons:
- âŒ Requires setup
- âŒ Still slower than cloud
- âŒ CPU/RAM intensive

**Recommendation: Best free local option**

---

### 5. **Faster Whisper (Local)** (MODERATE)
**Speed**: 250-500ms (1.5-2.5x faster than Vosk)
**Accuracy**: 95%+

#### Pros:
- âœ… Free
- âœ… Offline
- âœ… CTranslate2 optimization
- âœ… 4x faster than original Whisper

#### Cons:
- âŒ Requires setup
- âŒ Still slower than cloud

---

## ðŸ“Š Speed Comparison

| Service | Latency | Accuracy | Cost/100 queries | Free Tier |
|---------|---------|----------|------------------|-----------|
| **Current (Vosk)** | 500-1000ms | 85% | $0 | Unlimited |
| **Deepgram** | 100-300ms | 95% | $0 | 200 hrs/mo |
| **OpenAI Whisper** | 200-500ms | 98% | $1.80/mo | None |
| **AssemblyAI** | 150-400ms | 95% | $0-0.78 | 5 hrs/mo |
| **Whisper.cpp** | 300-600ms | 95% | $0 | Unlimited |
| **Faster Whisper** | 250-500ms | 95% | $0 | Unlimited |

**AI Focus Usage Estimate**: ~100 queries/day = ~5 hours/month

---

## ðŸš€ Implementation Options

### Option A: Deepgram (Recommended)

#### Installation:
```bash
pip install deepgram-sdk
```

#### Implementation:
```python
from deepgram import Deepgram
import asyncio

async def transcribe_with_deepgram(audio_bytes: bytes) -> str:
    """Transcribe audio using Deepgram API."""
    dg_client = Deepgram(DEEPGRAM_API_KEY)
    
    source = {'buffer': audio_bytes, 'mimetype': 'audio/webm'}
    
    response = await dg_client.transcription.prerecorded(
        source,
        {
            'punctuate': True,
            'model': 'nova-2',  # Latest, fastest model
            'language': 'en-US',
        }
    )
    
    transcript = response['results']['channels'][0]['alternatives'][0]['transcript']
    return transcript
```

#### Benefits:
- **3-5x faster** than Vosk
- Streaming support (get partial results immediately)
- Better accuracy
- Free for AI Focus usage levels

---

### Option B: OpenAI Whisper API

#### Installation:
```bash
pip install openai
```

#### Implementation:
```python
from openai import OpenAI

async def transcribe_with_whisper(audio_bytes: bytes) -> str:
    """Transcribe audio using OpenAI Whisper API."""
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    # Save to temp file
    with tempfile.NamedTemporaryFile(suffix='.webm', delete=False) as f:
        f.write(audio_bytes)
        temp_path = f.name
    
    try:
        with open(temp_path, 'rb') as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language="en"
            )
        return transcript.text
    finally:
        os.unlink(temp_path)
```

---

### Option C: Faster Whisper (Local, Free)

#### Installation:
```bash
pip install faster-whisper
```

#### Implementation:
```python
from faster_whisper import WhisperModel

# Initialize once (load model at startup)
model = WhisperModel("base.en", device="cpu", compute_type="int8")

async def transcribe_with_faster_whisper(audio_bytes: bytes) -> str:
    """Transcribe audio using Faster Whisper (local)."""
    # Save to temp file
    with tempfile.NamedTemporaryFile(suffix='.webm', delete=False) as f:
        f.write(audio_bytes)
        temp_path = f.name
    
    try:
        segments, info = model.transcribe(temp_path, beam_size=5)
        transcript = " ".join([segment.text for segment in segments])
        return transcript.strip()
    finally:
        os.unlink(temp_path)
```

---

## ðŸŽ¯ Recommended Implementation

### Hybrid Approach (Best of Both Worlds)

```python
@app.post("/api/transcribe")
async def transcribe_audio(file: UploadFile = File(...)):
    """
    Transcribe audio with fallback chain:
    1. Deepgram (fastest, cloud)
    2. Faster Whisper (fast, local)
    3. Vosk (fallback, local)
    """
    content = await file.read()
    
    # Try Deepgram first (if API key available)
    try:
        from config.api_key_loader import load_api_keys
        api_keys = await load_api_keys()
        deepgram_key = api_keys.get("deepgram", {}).get("api_key")
        
        if deepgram_key:
            logger.info("[TRANSCRIBE] Using Deepgram")
            start = time.time()
            transcript = await transcribe_with_deepgram(content, deepgram_key)
            elapsed = (time.time() - start) * 1000
            logger.info(f"[TRANSCRIBE] Deepgram completed in {elapsed:.0f}ms")
            
            if transcript:
                return {
                    "success": True,
                    "transcript": transcript,
                    "model_used": "deepgram",
                    "latency_ms": elapsed
                }
    except Exception as e:
        logger.warning(f"[TRANSCRIBE] Deepgram failed: {e}")
    
    # Fallback to Faster Whisper (local)
    try:
        logger.info("[TRANSCRIBE] Using Faster Whisper (local)")
        start = time.time()
        transcript = await transcribe_with_faster_whisper(content)
        elapsed = (time.time() - start) * 1000
        logger.info(f"[TRANSCRIBE] Faster Whisper completed in {elapsed:.0f}ms")
        
        if transcript:
            return {
                "success": True,
                "transcript": transcript,
                "model_used": "faster-whisper",
                "latency_ms": elapsed
            }
    except Exception as e:
        logger.warning(f"[TRANSCRIBE] Faster Whisper failed: {e}")
    
    # Final fallback to Vosk (current implementation)
    logger.info("[TRANSCRIBE] Using Vosk (fallback)")
    # ... existing Vosk code ...
```

---

## ðŸ“ˆ Expected Performance Improvements

### Current Performance:
```
Silence Detection:     1200ms
Transcription:         500-1000ms  â† TARGET
AI Generation:         ~2000ms
TTS:                   ~1000ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:                 ~5-6 seconds
```

### With Deepgram:
```
Silence Detection:     1200ms
Transcription:         100-300ms  â† 400-700ms FASTER!
AI Generation:         ~2000ms
TTS:                   ~1000ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:                 ~4-5 seconds
TIME SAVED:            400-700ms (8-14% improvement)
```

### With Faster Whisper (Free):
```
Silence Detection:     1200ms
Transcription:         250-500ms  â† 250-500ms FASTER!
AI Generation:         ~2000ms
TTS:                   ~1000ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:                 ~4.5-5.5 seconds
TIME SAVED:            250-500ms (5-10% improvement)
```

---

## ðŸ”§ Implementation Steps

### Phase 1: Add Deepgram (Recommended First)

1. **Get API Key**
   ```bash
   # Sign up at https://console.deepgram.com/
   # Free tier: 200 hours/month
   ```

2. **Install Package**
   ```bash
   pip install deepgram-sdk
   ```

3. **Update requirements.txt**
   ```
   deepgram-sdk>=3.0.0
   ```

4. **Add to API Keys Config**
   ```json
   {
     "deepgram": {
       "api_key": "your-deepgram-key"
     }
   }
   ```

5. **Update /api/transcribe Endpoint**
   - Add Deepgram as primary option
   - Keep Vosk as fallback
   - Add latency logging

### Phase 2: Add Faster Whisper (Optional, Local)

1. **Install Package**
   ```bash
   pip install faster-whisper
   ```

2. **Download Model** (one-time)
   ```python
   # Model auto-downloads on first use (~140MB for base.en)
   from faster_whisper import WhisperModel
   model = WhisperModel("base.en", device="cpu", compute_type="int8")
   ```

3. **Add as Second Fallback**
   - Faster Whisper before Vosk
   - Only if Deepgram unavailable

---

## ðŸ’° Cost Analysis

### AI Focus Usage (100 queries/day)
- **Query length**: ~3-5 seconds
- **Total audio/day**: 5-8 minutes
- **Total audio/month**: 150-240 minutes = 2.5-4 hours

### Deepgram Cost:
- Free tier: 200 hours/month
- Usage: ~4 hours/month
- **Cost: $0/month** (within free tier)

### OpenAI Whisper Cost:
- $0.006/minute
- Usage: ~200 minutes/month
- **Cost: ~$1.20/month**

### Recommendation:
**Use Deepgram** - stays within free tier, fastest option

---

## ðŸŽ¯ Quick Win Implementation

### Minimal Changes (15 minutes)

1. Add Deepgram to requirements:
   ```bash
   pip install deepgram-sdk
   ```

2. Add API key to database (Settings page)

3. Modify `/api/transcribe` to try Deepgram first

**Expected Result**: 400-700ms faster transcription immediately!

---

## ðŸ§ª Testing

### Measure Current Speed:
```python
# In web/main.py, add logging:
start = time.time()
transcript = await transcribe_audio(...)
elapsed = (time.time() - start) * 1000
logger.info(f"[TRANSCRIBE] Completed in {elapsed:.0f}ms")
```

### Compare Services:
```python
# Test script
import time

# Test Vosk
start = time.time()
vosk_result = transcribe_vosk(audio)
vosk_time = time.time() - start

# Test Deepgram
start = time.time()
deepgram_result = transcribe_deepgram(audio)
deepgram_time = time.time() - start

print(f"Vosk: {vosk_time * 1000:.0f}ms")
print(f"Deepgram: {deepgram_time * 1000:.0f}ms")
print(f"Speedup: {vosk_time / deepgram_time:.1f}x")
```

---

## ðŸ“š Documentation Updates Needed

1. Update `AI_FOCUS_OPTIMIZATION_PLAN.md`
2. Update `AI_FOCUS_TEST_REPORT.md` with new targets
3. Add transcription service comparison
4. Update performance metrics

---

## âœ… Recommendation

**Start with Deepgram:**
- âœ… 3-5x faster than Vosk
- âœ… Free for your usage level
- âœ… Easy to implement
- âœ… Best accuracy
- âœ… Can add streaming later (even faster!)

**Fallback to Vosk:**
- Keep existing code as fallback
- Zero breaking changes
- Gradual migration

**Expected Total Improvement:**
- Current: 3-5 seconds total
- With Deepgram: **2.5-4.5 seconds total**
- **Additional 10-15% speed boost**
- **Total improvement from start: 70-75% faster!**

---

## ðŸš€ Next Steps

1. Sign up for Deepgram (free tier)
2. Add API key to settings
3. Install deepgram-sdk
4. Implement hybrid transcription endpoint
5. Test and measure improvements
6. Update documentation

**Implementation time: ~30 minutes**
**Expected speedup: 400-700ms** (8-14% total improvement)
